{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shahidikram0701/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import api_client\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import json\n",
    "from time import time\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model variables\n",
    "n_hidden = 50\n",
    "num_dense = 25\n",
    "gradient_clipping_norm = 1.25\n",
    "# batch_size = 64\n",
    "# n_epoch = 25\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_NB_WORDS = 5000\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debug(*args):\n",
    "    if(DEBUG):\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_list_as_pickle(save_list, sample_size):\n",
    "    name = \"kaggle_data_\" + str(sample_size)+\".pickle\"\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(save_list, f)\n",
    "\n",
    "def load_list_from_pickle(sample_size):\n",
    "    filename = \"kaggle_data_\" + str(sample_size)+\".pickle\" \n",
    "    with open(filename, 'rb') as f:\n",
    "        mynewlist = pickle.load(f)\n",
    "    return mynewlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(size):\n",
    "    client = api_client.ApiClient(\"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE1NTQ2MjcxMDMsImlhdCI6MTUzOTA3NTEwMywibmJmIjoxNTM5MDc1MTAzLCJpZGVudGl0eSI6OX0.I4e6eJThErY_nznQUAJqOWKYnY0Z46WquZnEHX3ygck\")\n",
    "    global_data = []\n",
    "    iterations = size // 10000\n",
    "    if size > 10000:\n",
    "        new_size = 10000\n",
    "        for i in range(iterations):\n",
    "            data = client.get_kaggle_quora_data(num_samples = new_size)\n",
    "            global_data.extend(data)\n",
    "    data = client.get_kaggle_quora_data(num_samples = (size % 10000))\n",
    "    global_data.extend(data)\n",
    "    \n",
    "    debug(\"no of records obtained : \", len(global_data))\n",
    "    # flatten the list\n",
    "    # flat_list = []\n",
    "    # for sublist in global_data:\n",
    "    #     for item in sublist:\n",
    "    #         flat_list.append(item)\n",
    "    #store the pickle\n",
    "    store_list_as_pickle(global_data, size)\n",
    "    return global_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData2(size):\n",
    "    with open('dataset.json') as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    return dataset[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab,file_path='glove.6B.100d.txt'):\n",
    "    embeddings_index = {}\n",
    "    f = open(file_path,encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    # prepare embedding matrix\n",
    "    num_words = min(MAX_NB_WORDS, len(vocab) + 1)\n",
    "    \n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in vocab.items():\n",
    "        if i >= MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros\n",
    "            embedding_matrix[i] = embedding_vector \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_maxlen(sequences):\n",
    "    return len(max(sequences, key = lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_dataset(data, size, split_ratio):\n",
    "    global pad_length\n",
    "\n",
    "    t = Tokenizer(lower = True)\n",
    "    all_questions = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    question1 = []\n",
    "    question2 = []\n",
    "    labels = []\n",
    "    for i in data:\n",
    "        q1 = i['question1']\n",
    "        q2 = i['question2']\n",
    "        q1 = ' '.join([word for word in q1.split() if word not in stop_words])\n",
    "        q2 = ' '.join([word for word in q2.split() if word not in stop_words])\n",
    "        question1.append(q1)\n",
    "        question2.append(q2)\n",
    "\n",
    "        labels.append(int(i['is_duplicate']))\n",
    "\n",
    "    debug(\"All questions\")\n",
    "    debug(len(all_questions))\n",
    "    \n",
    "    all_questions = question1 + question2\n",
    "\n",
    "    t.fit_on_texts(all_questions)\n",
    "    vocab_temp = t.word_index\n",
    "    # debug(\"vocab_temp: \", vocab_temp)\n",
    "    i = 0\n",
    "    words = list(vocab_temp.keys()) \n",
    "    vocab = {}\n",
    "    while(i < size):\n",
    "        vocab[words[i]] = vocab_temp[words[i]]\n",
    "        i += 1\n",
    "\n",
    "    debug(\"vocabulory size\")\n",
    "    debug(len(vocab))\n",
    "\n",
    "    pad_length = get_maxlen(all_questions)\n",
    "    debug(\"pad_length = \", pad_length)\n",
    "\n",
    "    question1_sequences = np.array(pad_sequences(t.texts_to_sequences(question1), maxlen = pad_length))\n",
    "    question2_sequences = np.array(pad_sequences(t.texts_to_sequences(question2), maxlen = pad_length))\n",
    "\n",
    "    temp = np.arange(len(data))\n",
    "    np.random.shuffle(temp)\n",
    "\n",
    "    test_data_size = int(split_ratio * len(data))\n",
    "\n",
    "\n",
    "    question1_train = question1_sequences[temp[:(len(data) - test_data_size)]]\n",
    "    question1_test = question1_sequences[temp[(len(data) - test_data_size):]]\n",
    "    question2_train = question2_sequences[temp[:(len(data) - test_data_size)]]\n",
    "    question2_test = question2_sequences[temp[(len(data) - test_data_size):]]\n",
    "    labels = np.array(labels)\n",
    "    labels_train = labels[temp[:(len(data) - test_data_size)]]\n",
    "    labels_test = labels[temp[(len(data) - test_data_size):]]\n",
    "\n",
    "    debug(\"DEBUG\")\n",
    "    debug(question1[temp[500]])\n",
    "    debug(question2[temp[500]])\n",
    "    debug(labels[500])\n",
    "\n",
    "    return vocab, question1_train, question2_train, question1_test, question2_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix):\n",
    "    max_seq_length = pad_length\n",
    "    def exponent_neg_manhattan_distance(left, right):\n",
    "        ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "        return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "    # The visible layer\n",
    "    left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "    right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(len(embedding_matrix),EMBEDDING_DIM, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "    # Embedded version of the inputs\n",
    "    encoded_left = embedding_layer(left_input)\n",
    "    encoded_right = embedding_layer(right_input)\n",
    "\n",
    "    # Since this is a siamese network, both sides share the same LSTM\n",
    "    shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "    left_output = shared_lstm(encoded_left)\n",
    "    right_output = shared_lstm(encoded_right)\n",
    "\n",
    "    #conactenated output\n",
    "    merged = concatenate([left_output,right_output])\n",
    "    merged = Dropout(rate = 0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    merged = Dense(num_dense, activation='elu')(merged)\n",
    "    merged = Dropout(rate=0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "    # Calculates the distance as defined by the MaLSTM model\n",
    "    malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "    # Pack it all up into a model\n",
    "    malstm = Model([left_input, right_input], [preds])\n",
    "\n",
    "    # Adadelta optimizer, with gradient clipping by norm\n",
    "    \n",
    "    return malstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(malstm, X_train_q1, X_train_q2, Y_train, batch_size = 16, n_epoch = 5):\n",
    "    optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "    malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # Start training\n",
    "    training_start_time = time()\n",
    "    malstm_trained = malstm.fit([X_train_q1, X_train_q2], Y_train, batch_size=batch_size, nb_epoch = n_epoch, validation_split = 0.2, verbose = 1)\n",
    "    print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\n",
    "    return malstm_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "[{'id': '0', 'qid1': '1', 'qid2': '2', 'question1': 'What is the step by step guide to invest in share market in india?', 'question2': 'What is the step by step guide to invest in share market?', 'is_duplicate': '0'}, {'id': '1', 'qid1': '3', 'qid2': '4', 'question1': 'What is the story of Kohinoor (Koh-i-Noor) Diamond?', 'question2': 'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?', 'is_duplicate': '0'}, {'id': '2', 'qid1': '5', 'qid2': '6', 'question1': 'How can I increase the speed of my internet connection while using a VPN?', 'question2': 'How can Internet speed be increased by hacking through DNS?', 'is_duplicate': '0'}]\n",
      "All questions\n",
      "0\n",
      "vocabulory size\n",
      "10000\n",
      "pad_length =  791\n",
      "DEBUG\n",
      "Which best travel planning website?\n",
      "What best website app use trip planning, why?\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahidikram0701/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22400 samples, validate on 5600 samples\n",
      "Epoch 1/5\n",
      "22400/22400 [==============================] - 2361s 105ms/step - loss: 0.2283 - acc: 0.6379 - val_loss: 0.2014 - val_acc: 0.6900\n",
      "Epoch 2/5\n",
      "22400/22400 [==============================] - 2375s 106ms/step - loss: 0.2046 - acc: 0.6832 - val_loss: 0.1949 - val_acc: 0.7059\n",
      "Epoch 3/5\n",
      "22400/22400 [==============================] - 2378s 106ms/step - loss: 0.1965 - acc: 0.6984 - val_loss: 0.1959 - val_acc: 0.6979\n",
      "Epoch 4/5\n",
      "22400/22400 [==============================] - 2383s 106ms/step - loss: 0.1930 - acc: 0.7064 - val_loss: 0.1903 - val_acc: 0.7086\n",
      "Epoch 5/5\n",
      "22400/22400 [==============================] - 2396s 107ms/step - loss: 0.1898 - acc: 0.7112 - val_loss: 0.1895 - val_acc: 0.7129\n",
      "Training time finished.\n",
      "5 epochs in 3:18:14.095165\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_size = 40000\n",
    "    vocab_size = 10000\n",
    "\n",
    "    # try:\n",
    "    #     data = load_list_from_pickle(data_size)\n",
    "    #     debug(\"read from pickle\")\n",
    "    # except:\n",
    "    #     data = getData(data_size)\n",
    "    \n",
    "    data = getData2(data_size)\n",
    "\n",
    "    debug(\"Dataset\")\n",
    "    debug(data[:3])\n",
    "\n",
    "    vocabulary, question1_train, question2_train, question1_test, question2_test, labels_train, labels_test = pre_process_dataset(data, vocab_size, split_ratio = 0.3)\n",
    "\n",
    "    # debug(\"Debuging train-test split!\")\n",
    "    # debug(question1_train[0])\n",
    "    # debug(question2_train[0])\n",
    "    # debug(labels_train[0])\n",
    "\n",
    "    embedding_matrix = create_embedding_matrix(vocabulary)\n",
    "\n",
    "    #debug(vocabulary)\n",
    "    #debug(embedding_matrix)\n",
    "\n",
    "    model = create_model(embedding_matrix)\n",
    "    trained_model = train_model(model, question1_train, question2_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "[{'id': '0', 'qid1': '1', 'qid2': '2', 'question1': 'What is the step by step guide to invest in share market in india?', 'question2': 'What is the step by step guide to invest in share market?', 'is_duplicate': '0'}, {'id': '1', 'qid1': '3', 'qid2': '4', 'question1': 'What is the story of Kohinoor (Koh-i-Noor) Diamond?', 'question2': 'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?', 'is_duplicate': '0'}, {'id': '2', 'qid1': '5', 'qid2': '6', 'question1': 'How can I increase the speed of my internet connection while using a VPN?', 'question2': 'How can Internet speed be increased by hacking through DNS?', 'is_duplicate': '0'}]\n",
      "All questions\n",
      "0\n",
      "vocabulory size\n",
      "10000\n",
      "pad_length =  791\n",
      "DEBUG\n",
      "What difference parliamentary presidential form government?\n",
      "Is presidential form government better alternative parliamentary form government?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data_size = 40000\n",
    "vocab_size = 10000\n",
    "\n",
    "# try:\n",
    "#     data = load_list_from_pickle(data_size)\n",
    "#     debug(\"read from pickle\")\n",
    "# except:\n",
    "#     data = getData(data_size)\n",
    "\n",
    "data = getData2(data_size)\n",
    "\n",
    "debug(\"Dataset\")\n",
    "debug(data[:3])\n",
    "\n",
    "vocabulary, question1_train, question2_train, question1_test, question2_test, labels_train, labels_test = pre_process_dataset(data, vocab_size, split_ratio = 0.3)\n",
    "\n",
    "# debug(\"Debuging train-test split!\")\n",
    "# debug(question1_train[0])\n",
    "# debug(question2_train[0])\n",
    "# debug(labels_train[0])\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocabulary)\n",
    "\n",
    "#debug(vocabulary)\n",
    "#debug(embedding_matrix)\n",
    "\n",
    "model = create_model(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 244s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18221123437086742, 0.7270833333333333]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([question1_test, question2_test], labels_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 791)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 791)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 791, 100)     500000      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50)           30200       embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100)          400         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 25)           2525        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 25)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 25)           100         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            26          batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 533,251\n",
      "Trainable params: 33,001\n",
      "Non-trainable params: 500,250\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
